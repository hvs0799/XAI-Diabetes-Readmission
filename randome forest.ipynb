{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74528f09-01d8-4d6c-b197-86a4a45f349b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harsha S\\AppData\\Local\\Temp\\ipykernel_34280\\2700521384.py:34: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['readmitted'] = data['readmitted'].replace({'>30': 0, 'NO': 0, '<30': 1})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.91     18082\n",
      "           1       0.95      0.86      0.90     18082\n",
      "\n",
      "    accuracy                           0.91     36164\n",
      "   macro avg       0.91      0.91      0.91     36164\n",
      "weighted avg       0.91      0.91      0.91     36164\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[17196   886]\n",
      " [ 2453 15629]]\n",
      "\n",
      "✅ Accuracy: 0.9077\n",
      "✅ Precision: 0.9464\n",
      "✅ Recall: 0.8643\n",
      "✅ F1 Score: 0.9035\n",
      "✅ ROC-AUC: 0.9548\n",
      "\n",
      "🔎 Running SHAP explainability (optimized)...\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 1. Import Libraries\n",
    "# -------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Load Data\n",
    "# -------------------------------\n",
    "data = pd.read_csv(\"diabetic_data.csv\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Cleaning\n",
    "# -------------------------------\n",
    "data = data.drop(['weight', 'payer_code', 'medical_specialty'], axis=1)\n",
    "data = data.replace(\"?\", np.nan)\n",
    "data = data.drop(['encounter_id', 'patient_nbr'], axis=1)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Target Variable\n",
    "# -------------------------------\n",
    "data['readmitted'] = data['readmitted'].replace({'>30': 0, 'NO': 0, '<30': 1})\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Encode Categorical Variables\n",
    "# -------------------------------\n",
    "categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "le = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    data[col] = le.fit_transform(data[col].astype(str))\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Features & Labels\n",
    "# -------------------------------\n",
    "X = data.drop('readmitted', axis=1)\n",
    "y = data['readmitted']\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Handle Class Imbalance with SMOTE\n",
    "# -------------------------------\n",
    "sm = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X, y)\n",
    "\n",
    "# -------------------------------\n",
    "# 8. Train-Test Split\n",
    "# -------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.2, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 9. Feature Scaling\n",
    "# -------------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# -------------------------------\n",
    "# 10. Train Random Forest\n",
    "# -------------------------------\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Save model & scaler for deployment\n",
    "joblib.dump(rf, \"rf_model.pkl\")\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "joblib.dump(X.columns.tolist(), \"feature_names.pkl\")\n",
    "\n",
    "# -------------------------------\n",
    "# 11. Evaluation\n",
    "# -------------------------------\n",
    "y_pred = rf.predict(X_test)\n",
    "y_prob = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(f\"\\n✅ Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"✅ Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"✅ Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"✅ F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"✅ ROC-AUC: {roc_auc_score(y_test, y_prob):.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 12. SHAP Explainability (Optimized for Speed)\n",
    "# -------------------------------\n",
    "# -------------------------------\n",
    "# 12. SHAP Explainability (Optimized)\n",
    "# -------------------------------\n",
    "print(\"\\n🔎 Running SHAP explainability (optimized)...\")\n",
    "\n",
    "# Convert scaled test/train back to DataFrames\n",
    "X_train_df = pd.DataFrame(X_train, columns=X.columns)\n",
    "X_test_df = pd.DataFrame(X_test, columns=X.columns)\n",
    "\n",
    "# Background dataset (small sample from train)\n",
    "background = X_train_df.sample(100, random_state=42)\n",
    "\n",
    "# Test sample (smaller for speed)\n",
    "X_sample = X_test_df.sample(50, random_state=42)\n",
    "\n",
    "# Create explainer with background dataset\n",
    "explainer = shap.TreeExplainer(rf, data=background)\n",
    "\n",
    "# Compute SHAP values (raw output is default and fastest)\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "# --- SHAP Summary Plot (Global Importance) ---\n",
    "shap.summary_plot(shap_values[1], X_sample, show=False)  # [1] for readmission=1\n",
    "plt.title(\"SHAP Summary Plot - Random Forest\")\n",
    "plt.savefig(\"shap_summary_rf.png\", bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# --- SHAP Waterfall Plot (Local Explanation for 1st sample) ---\n",
    "shap.plots._waterfall.waterfall_legacy(\n",
    "    explainer.expected_value[1], shap_values[1][0], X_sample.iloc[0], show=False\n",
    ")\n",
    "plt.savefig(\"shap_waterfall_rf.png\", bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(\"✅ SHAP plots saved: shap_summary_rf.png & shap_waterfall_rf.png (fast mode)\")\n",
    "\n",
    "# -------------------------------\n",
    "# 13. Streamlit Dashboard\n",
    "# -------------------------------\n",
    "# Save this code separately in `app.py` and run: streamlit run app.py\n",
    "\"\"\"\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Load model, scaler, and features\n",
    "model = joblib.load(\"rf_model.pkl\")\n",
    "scaler = joblib.load(\"scaler.pkl\")\n",
    "feature_names = joblib.load(\"feature_names.pkl\")\n",
    "\n",
    "st.title(\"🏥 Diabetes Readmission Prediction Dashboard\")\n",
    "\n",
    "st.write(\"Enter patient details to predict hospital readmission:\")\n",
    "\n",
    "# Input fields\n",
    "inputs = {}\n",
    "for col in feature_names:\n",
    "    inputs[col] = st.number_input(f\"{col}\", value=0.0)\n",
    "\n",
    "# Convert to DataFrame\n",
    "input_df = pd.DataFrame([inputs])\n",
    "\n",
    "# Scale\n",
    "input_scaled = scaler.transform(input_df)\n",
    "\n",
    "# Prediction\n",
    "pred_prob = model.predict_proba(input_scaled)[0][1]\n",
    "prediction = model.predict(input_scaled)[0]\n",
    "\n",
    "st.subheader(\"Prediction Result\")\n",
    "if prediction == 1:\n",
    "    st.error(f\"⚠️ Patient likely to be readmitted (probability: {pred_prob:.2f})\")\n",
    "else:\n",
    "    st.success(f\"✅ Patient unlikely to be readmitted (probability: {pred_prob:.2f})\")\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
