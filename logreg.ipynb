{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24a061ab-e2af-4d2a-bc30-5405a4456914",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SMOTE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 59\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m     57\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m     58\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m'\u001b[39m, StandardScaler()),\n\u001b[1;32m---> 59\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmote\u001b[39m\u001b[38;5;124m'\u001b[39m, SMOTE(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)),\n\u001b[0;32m     60\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclf\u001b[39m\u001b[38;5;124m'\u001b[39m, GridSearchCV(\n\u001b[0;32m     61\u001b[0m         LogisticRegression(class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m'\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m),\n\u001b[0;32m     62\u001b[0m         param_grid\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m     63\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m],\n\u001b[0;32m     64\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpenalty\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml2\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     65\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msolver\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlbfgs\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     66\u001b[0m         },\n\u001b[0;32m     67\u001b[0m         scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     68\u001b[0m         cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[0;32m     69\u001b[0m     ))\n\u001b[0;32m     70\u001b[0m ])\n\u001b[0;32m     72\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     73\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SMOTE' is not defined"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 1. Import Libraries\n",
    "# -------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Load Data\n",
    "# -------------------------------\n",
    "data = pd.read_csv(\"diabetic_data.csv\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Initial Cleaning\n",
    "# -------------------------------\n",
    "# Drop columns with too many missing or irrelevant values\n",
    "data = data.drop(['weight', 'payer_code', 'medical_specialty'], axis=1)\n",
    "\n",
    "# Replace \"?\" with NaN\n",
    "data = data.replace(\"?\", np.nan)\n",
    "\n",
    "# Drop encounter_id and patient_nbr (identifiers, not useful for prediction)\n",
    "data = data.drop(['encounter_id', 'patient_nbr'], axis=1)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Target Variable\n",
    "# -------------------------------\n",
    "# Convert 'readmitted' into binary classification: 1 (readmitted within 30 days) vs 0 (not readmitted)\n",
    "data['readmitted'] = data['readmitted'].replace({'>30': 0, 'NO': 0, '<30': 1})\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Encode Categorical Variables\n",
    "# -------------------------------\n",
    "categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "le = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    data[col] = data[col].astype(str)  # ensure string type\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Split Features & Labels\n",
    "# -------------------------------\n",
    "X = data.drop('readmitted', axis=1)\n",
    "y = data['readmitted']\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Handle Imbalance using SMOTE\n",
    "# -------------------------------\n",
    "sm = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X, y)\n",
    "\n",
    "print(\"Original class distribution:\\n\", y.value_counts())\n",
    "print(\"\\nAfter SMOTE balancing:\\n\", y_res.value_counts())\n",
    "\n",
    "# -------------------------------\n",
    "# 8. Train-Test Split\n",
    "# -------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.2, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 9. Feature Scaling\n",
    "# -------------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"âœ… Data Preprocessing & Balancing Complete\")\n",
    "print(\"Train Shape:\", X_train.shape, \" Test Shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b846c79-b66c-4887-a5a3-d0d8de32c478",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
